{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSGZ6cdmpknm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/snntorch_alpha_w.png?raw=true' width=\"400\">](https://github.com/jeshraghian/snntorch/)\n",
    "\n",
    "\n",
    "# snnTorch - Training Spiking Convolutional Neural Networks with snnTorch\n",
    "## Tutorial 5\n",
    "### By Jason K. Eshraghian (www.jasoneshraghian.com)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/master/examples/tutorial_6_CNN.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "\n",
    "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub-Mark-Light-120px-plus.png?raw=true' width=\"28\">](https://github.com/jeshraghian/snntorch/) [<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub_Logo_White.png?raw=true' width=\"80\">](https://github.com/jeshraghian/snntorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rugeYYiqsrlc"
   },
   "source": [
    "The snnTorch tutorial series is based on the following paper. If you find these resources or code useful in your work, please consider citing the following source:\n",
    "\n",
    "> <cite> [Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. \"Training Spiking Neural Networks Using Lessons From Deep Learning\". arXiv preprint arXiv:2109.12894, September 2021.](https://arxiv.org/abs/2109.12894) </cite>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ymi3sqJg28OQ"
   },
   "source": [
    "# Introduction\n",
    "In this tutorial, you will:\n",
    "* Learn how spiking neurons are implemented as a recurrent network\n",
    "* Understand backpropagation through time, and the associated challenges in SNNs such as the non-differentiability of spikes\n",
    "* Train a fully-connected network on the static MNIST dataset\n",
    "\n",
    "<!-- * Implement various backprop strategies:\n",
    "  * Backpropagation Through Time\n",
    "  * Truncated-Backpropagation Through Time\n",
    "  * Real-Time Recurrent Learning -->\n",
    "\n",
    ">Part of this tutorial was inspired by Friedemann Zenke's extensive work on SNNs. Check out his repo on surrogate gradients [here](https://github.com/fzenke/spytorch), and a favourite paper of mine: E. O. Neftci, H. Mostafa, F. Zenke, [Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-based optimization to spiking neural networks.](https://ieeexplore.ieee.org/document/8891809) IEEE Signal Processing Magazine 36, 51–63.\n",
    "\n",
    "At the end of the tutorial, a basic supervised learning algorithm will be implemented. We will use the original static MNIST dataset and train a multi-layer fully-connected spiking neural network using gradient descent to perform image classification. \n",
    "\n",
    "If running in Google Colab:\n",
    "* You may connect to GPU by checking `Runtime` > `Change runtime type` > `Hardware accelerator: GPU`\n",
    "* Next, install the latest PyPi distribution of snnTorch by clicking into the following cell and pressing `Shift+Enter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1657784286329,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "QXZ6Tuqc9Q-l",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/export/isn/keli/.cache/pypoetry/virtualenvs/l2s-qKZNf-9r-py3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Volumes/export/isn/keli/.cache/pypoetry/virtualenvs/l2s-qKZNf-9r-py3.9/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /Volumes/export/isn/keli/.cache/pypoetry/virtualenvs/l2s-qKZNf-9r-py3.9/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gt2xMbLY9dVE"
   },
   "source": [
    "# 1. A Recurrent Representation of SNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7haBG7nA_TC"
   },
   "source": [
    "In Tutorial 3, we derived a recursive representation of a leaky integrate-and-fire (LIF) neuron: \n",
    "\n",
    "$$U[t+1] = \\underbrace{\\beta U[t]}_\\text{decay} + \\underbrace{WX[t+1]}_\\text{input} - \\underbrace{R[t]}_\\text{reset} \\tag{1}$$\n",
    "\n",
    "where the input synaptic current is interpreted as $I_{\\rm in}[t] = WX[t]$, and $X[t]$ may be some arbitrary input of spikes, a step/time-varying voltage, or unweighted step/time-varying current. Spiking is represented with the following equation, where if the membrane potential exceeds the threshold, a spike is emitted:\n",
    "\n",
    "$$S[t] = \\begin{cases} 1, &\\text{if}~U[t] > U_{\\rm thr} \\\\\n",
    "0, &\\text{otherwise}\\end{cases} \\tag{2}$$\n",
    "\n",
    "This formulation of a spiking neuron in a discrete, recursive form is almost perfectly poised to take advantage of the developments in training recurrent neural networks (RNNs) and sequence-based models. This is illustrated using an *implicit* recurrent connection for the decay of the membrane potential, and is distinguished from *explicit* recurrence where the output spike $S_{\\rm out}$ is fed back to the input. In the figure below, the connection weighted by $-U_{\\rm thr}$ represents the reset mechanism $R[t]$.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial5/unrolled_2.png?raw=true' width=\"800\">\n",
    "</center>\n",
    "\n",
    "The benefit of an unrolled graph is that it provides an explicit description of how computations are performed. The process of unfolding illustrates the flow of information forward in time (from left to right) to compute outputs and losses, and backward in time to compute gradients. The more time steps that are simulated, the deeper the graph becomes. \n",
    "\n",
    "Conventional RNNs treat $\\beta$ as a learnable parameter. This is also possible for SNNs, though by default, they are treated as hyperparameters. This replaces the vanishing and exploding gradient problems with a hyperparameter search. A future tutorial will describe how to make $\\beta$ a learnable parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wgzf83HE2BeB"
   },
   "source": [
    "# 2. The Non-Differentiability of Spikes\n",
    "## 2.1 Training Using the Backprop Algorithm\n",
    "\n",
    "An alternative way to represent the relationship between $S$ and $U$ in $(2)$ is:\n",
    "\n",
    "$$S[t] = \\Theta(U[t] - U_{\\rm thr}) \\tag{3}$$ \n",
    "\n",
    "where $\\Theta(\\cdot)$ is the Heaviside step function:\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/3_2_spike_descrip.png?raw=true' width=\"600\">\n",
    "</center>\n",
    "\n",
    "Training a network in this form poses some serious challenges. Consider a single, isolated time step of the computational graph from the previous figure titled *\"Recurrent representation of spiking neurons\"*, as shown in the *forward pass* below:\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial5/non-diff.png?raw=true' width=\"400\">\n",
    "</center>\n",
    "\n",
    "The goal is to train the network using the gradient of the loss with respect to the weights, such that the weights are updated to minimize the loss. The backpropagation algorithm achieves this using the chain rule:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial S}\n",
    "\\underbrace{\\frac{\\partial S}{\\partial U}}_{\\{0, \\infty\\}}\n",
    "\\frac{\\partial U}{\\partial I}\\\n",
    "\\frac{\\partial I}{\\partial W}\\ \\tag{4}$$\n",
    "\n",
    "From $(1)$, $\\partial I/\\partial W=X$, and $\\partial U/\\partial I=1$. While we have not yet defined a loss function, we can assume $\\partial \\mathcal{L}/\\partial S$ has an analytical solution, in a similar form to the cross-entropy or mean-square error loss (more on that shortly). \n",
    "\n",
    "However, the term that we are going to grapple with is $\\partial S/\\partial U$. The derivative of the Heaviside step function from $(3)$ is the Dirac Delta function, which evaluates to 0 everywhere, except at the threshold $U_{\\rm thr} = \\theta$, where it tends to infinity. This means the gradient will almost always be nulled to zero (or saturated if $U$ sits precisely at the threshold), and no learning can take place. This is known as the **dead neuron problem**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVrM7nLOMvgx"
   },
   "source": [
    "## 2.2 Overcoming the Dead Neuron Problem\n",
    "\n",
    "The most common way to address the dead neuron problem is to keep the Heaviside function as it is during the forward pass, but swap the derivative term $\\partial S/\\partial U$ for something that does not kill the learning process during the backward pass, which will be denoted $\\partial \\tilde{S}/\\partial U$. This might sound odd, but it turns out that neural networks are quite robust to such approximations. This is commonly known as the *surrogate gradient* approach.\n",
    "\n",
    "A variety of options exist to using surrogate gradients, and we will dive into more detail on these methods in [Tutorial 6](https://snntorch.readthedocs.io/en/latest/tutorials/index.html). For now, a simple approximation is applied where $\\partial \\tilde{S}/\\partial U$ is set to $S$ itself.\n",
    "\n",
    "If $S$ does not spike, then the spike-gradient term is $0$. If $S$ spikes, then the gradient term is $1$. This simply looks like the gradient of a ReLU function shifted to the threshold. This method is known as the *Spike-Operator* approach and is described in more detail in the following paper:\n",
    "\n",
    "> <cite> Jason K. Eshraghian, Max Ward, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. \"Training Spiking Neural Networks Using Lessons From Deep Learning\". arXiv, 2021. </cite>\n",
    "\n",
    "Inutitively, *Spike Operator* splits the gradient calculation into two chunks: one where the neuron is spiking, and one where it is silent:\n",
    "* **Silent:** If the neuron is silent, then the spike response can be obtained by scaling the membrane by 0: $S = U \\times 0 \\implies \\partial \\tilde{S}/\\partial U = 0$. \n",
    "* **Spiking:** If the neuron is spiking, then assume $U \\approx U_{\\rm thr}$, normalize $U_{\\rm thr}=1$, and the spike response can be obtained by scaling the membrane by 1: $S = U \\times 1 \\implies \\partial \\tilde{S}/\\partial U = 1$, where the tilde above $\\tilde{S}$ implies an approximation.\n",
    "\n",
    "This is summarized as follows:\n",
    "\n",
    "$$\\frac{\\partial \\tilde{S}}{\\partial U} \\leftarrow S = \\begin{cases} 1, &\\text{if}~U> U_{\\rm thr} \\\\\n",
    "0, &\\text{otherwise}\\end{cases} $$\n",
    "\n",
    "where the left arrow denotes substitution. \n",
    "\n",
    "The same neuron model described in  $(1)-(2)$ (a.k.a., `snn.Leaky` neuron from Tutorial 3) is implemented in PyTorch below. Don't worry if you don't understand this. This will be condensed into one line of code using snnTorch in a moment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1657784286329,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "mfJUm-6T8aG2"
   },
   "outputs": [],
   "source": [
    "# Leaky neuron model, overriding the backward pass with a custom function\n",
    "class LeakySurrogate(nn.Module):\n",
    "  def __init__(self, beta, threshold=1.0):\n",
    "      super(LeakySurrogate, self).__init__()\n",
    "\n",
    "      # initialize decay rate beta and threshold\n",
    "      self.beta = beta\n",
    "      self.threshold = threshold\n",
    "      self.spike_op = self.SpikeOperator.apply\n",
    "  \n",
    "  # the forward function is called each time we call Leaky\n",
    "  def forward(self, input_, mem):\n",
    "    spk = self.spike_op((mem-self.threshold))  # call the Heaviside function\n",
    "    reset = (spk * self.threshold).detach() # removes spike_op gradient from reset\n",
    "    mem = self.beta * mem + input_ - reset # Eq (1)\n",
    "    return spk, mem\n",
    "\n",
    "  # Forward pass: Heaviside function\n",
    "  # Backward pass: Override Dirac Delta with the Spike itself\n",
    "  @staticmethod\n",
    "  class SpikeOperator(torch.autograd.Function):\n",
    "      @staticmethod\n",
    "      def forward(ctx, mem):\n",
    "          spk = (mem > 0).float() # Heaviside on the forward pass: Eq(2)\n",
    "          ctx.save_for_backward(spk)  # store the spike for use in the backward pass\n",
    "          return spk\n",
    "\n",
    "      @staticmethod\n",
    "      def backward(ctx, grad_output):\n",
    "          (spk,) = ctx.saved_tensors  # retrieve the spike \n",
    "          grad = grad_output * spk # scale the gradient by the spike: 1/0\n",
    "          return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zkc1Mmp97OX"
   },
   "source": [
    "Note that the reset mechanism is detached from the computational graph, as the surrogate gradient should only be applied to $\\partial S/\\partial U$, and not $\\partial R/\\partial U$.\n",
    "\n",
    "The above neuron is instantiated using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1657784286598,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "EV3lU6soOnW6"
   },
   "outputs": [],
   "source": [
    "lif1 = LeakySurrogate(beta=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StklvL_gPns1"
   },
   "source": [
    "This neuron can be simulated using a for-loop, just as in previous tutorials, while PyTorch's automatic differentation (autodiff) mechanism keeps track of the gradient in the background.\n",
    "\n",
    "Alternatively, the same thing can be accomplished by calling the `snn.Leaky` neuron. \n",
    "In fact, every time you call any neuron model from snnTorch, the *Spike Operator* surrogate gradient is applied to it by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1657784286598,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "8Wa7N31mP9Va"
   },
   "outputs": [],
   "source": [
    "#beta is set to zero\n",
    "lif1 = snn.Leaky(beta=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EF70Xi1RX6w"
   },
   "source": [
    "If you would like to explore how this neuron behaves, then refer to [Tutorial 3](https://snntorch.readthedocs.io/en/latest/tutorials/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxl1UYSCRzzl"
   },
   "source": [
    "#3. Backprop Through Time\n",
    "Equation $(4)$ only calculates the gradient for one single time step (referred to as the *immediate influence* in the figure below), but the backpropagation through time (BPTT) algorithm calculates the gradient from the loss to *all* descendants and sums them together. \n",
    "\n",
    "The weight $W$ is applied at every time step, and so imagine a loss is also calculated at every time step. The influence of the weight on present and historical losses must be summed together to define the global gradient:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W}=\\sum_t \\frac{\\partial\\mathcal{L}[t]}{\\partial W} = \n",
    "\\sum_t \\sum_{s\\leq t} \\frac{\\partial\\mathcal{L}[t]}{\\partial W[s]}\\frac{\\partial W[s]}{\\partial W} \\tag{5} $$\n",
    "\n",
    "The point of $(5)$ is to ensure causality: by constraining $s\\leq t$, we only account for the contribution of immediate and prior influences of $W$ on the loss. A recurrent system constrains the weight to be shared across all steps: $W[0]=W[1] =~... ~ = W$. Therefore, a change in $W[s]$ will have the same effect on all $W$, which implies that $\\partial W[s]/\\partial W=1$:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W}=\n",
    "\\sum_t \\sum_{s\\leq t} \\frac{\\partial\\mathcal{L}[t]}{\\partial W[s]} \\tag{6} $$\n",
    "\n",
    "As an example, isolate the prior influence due to $s = t-1$ *only*; this means the backward pass must track back in time by one step. The influence of $W[t-1]$ on the loss can be written as:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}[t]}{\\partial W[t-1]} = \n",
    "\\frac{\\partial \\mathcal{L}[t]}{\\partial S[t]}\n",
    "\\underbrace{\\frac{\\partial \\tilde{S}[t]}{\\partial U[t]}}_{S[t]}\n",
    "\\underbrace{\\frac{\\partial U[t]}{\\partial U[t-1]}}_\\beta\n",
    "\\underbrace{\\frac{\\partial U[t-1]}{\\partial I[t-1]}}_1\n",
    "\\underbrace{\\frac{\\partial I[t-1]}{\\partial W[t-1]}}_{X[t-1]} \\tag{7}$$\n",
    "\n",
    "We have already dealt with all of these terms from $(4)$, except for $\\partial U[t]/\\partial U[t-1]$. From $(1)$, this temporal derivative term simply evaluates to $\\beta$. So if we really wanted to, we now know enough to painstakingly calculate the derivative of every weight at every time step by hand, and it'd look something like this for a single neuron:\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial5/bptt.png?raw=true' width=\"600\">\n",
    "</center>\n",
    "\n",
    "But thankfully, PyTorch's autodiff takes care of that in the background for us.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_42-CbsZ1FM"
   },
   "source": [
    "Grip turns a necessity into a fashion statement. And it still works like the high quality Fleck whips that you love! If you are looking for an English riding whip with the security of the Fleck Grip Handle but you also love a bit of sparkle in your life, you're sure to love the Fleck Glitter Riding whip.\n",
    "\n",
    "Available in pink, lilac and blue, this stylish riding crop measures 45 inches or 60 cm. These Fleck whips are grouped as \"Assorted\" - if you have a color preference, please note that in the Comments field when you check out. Just # 4. Setting up the Loss / Output Decoding\n",
    "In a conventional, non-spiking neural network, a supervised, multi-class classification problem takes the neuron with the highest activation and treats that as the predicted class. \n",
    "\n",
    "In a spiking neural net, there are several options to interpreting the output spikes. The most common approaches are:\n",
    "* **Rate coding:** Take the neuron with the highest firing rate (or spike count) as the predicted class\n",
    "* **Latency coding:** Take the neuron that fires *first* as the predicted class\n",
    "\n",
    "This might feel familiar to [Tutorial 1 on neural encoding](https://snntorch.readthedocs.io/en/latest/tutorials/index.html). The difference is that, here, we are interpreting (decoding) the output spikes, rather than encoding/converting raw input data into spikes.\n",
    "\n",
    "Let's focus on a rate code. When input data is passed to the network, we want the correct neuron class to emit the most spikes over the course of the simulation run. This then corresponds to the highest average firing frequency. One way to achieve this is to increase the membrane potential of the correct class to $U>U_{\\rm thr}$, and that of incorrect classes to $U<U_{\\rm thr}$. Applying the target to $U$ serves as a proxy for modulating spiking behavior from $S$.\n",
    "\n",
    "This can be implemented by taking the softmax of the membrane potential for output neurons, where $C$ is the number of output classes:\n",
    "\n",
    "$$p_i[t] = \\frac{e^{U_i[t]}}{\\sum_{i=0}^{C}e^{U_i[t]}} \\tag{8}$$\n",
    "\n",
    "The cross-entropy between $p_i$ and the target $y_i \\in \\{0,1\\}^C$, which is a one-hot target vector, is obtained using:\n",
    "\n",
    "$$\\mathcal{L}_{CE}[t] = \\sum_{i=0}^Cy_i{\\rm log}(p_i[t]) \\tag{9}$$\n",
    "\n",
    "The practical effect is that the membrane potential of the correct class is encouraged to increase while those of incorrect classes are reduced. In effect, this means the correct class is encouraged to fire at all time steps, while incorrect classes are suppressed at all steps. This may not be the most efficient implementation of an SNN, but it is among the simplest.\n",
    "\n",
    "This target is applied at every time step of the simulation, thus also generating a loss at every step. These losses are then summed together at the end of the simulation:\n",
    "\n",
    "$$\\mathcal{L}_{CE} = \\sum_t\\mathcal{L}_{CE}[t] \\tag{10}$$\n",
    "\n",
    "This is just one of many possible ways to apply a loss function to a spiking neural network. A variety of approaches are available to use in snnTorch (in the module `snn.functional`), and will be the subject of a future tutorial.\n",
    "\n",
    "With all of the background theory having been taken care of, let’s finally dive into\n",
    "training a fully-connected spiking neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqJdfllYbc16"
   },
   "source": [
    "# 5. Setting up the Static MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1657784286599,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "lI0GbgLgpkos",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataloader arguments\n",
    "batch_size = 128\n",
    "data_path='~/justinData/mnistData'\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1657784286599,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "2fhRixcspkot",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAM_dP887uTq"
   },
   "source": [
    "If the above code blocks throws an error, e.g. the MNIST servers are down, then uncomment the following code instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1657784286600,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "4jyJVqUNdXDo"
   },
   "outputs": [],
   "source": [
    "# # temporary dataloader if MNIST service is unavailable\n",
    "# !wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "# !tar -zxvf MNIST.tar.gz\n",
    "\n",
    "# mnist_train = datasets.MNIST(root = './', train=True, download=True, transform=transform)\n",
    "# mnist_test = datasets.MNIST(root = './', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1657784286600,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "aEtCbO6upkou",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhFyzySNeT_e"
   },
   "source": [
    "# 6. Define the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1657784286784,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "Lud3kywn55fj"
   },
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = 28*28\n",
    "num_hidden = 1000\n",
    "num_outputs = 10\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 25\n",
    "#beta is set to 0.0\n",
    "beta = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1657784286785,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "-uquHLLmpkox",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        \n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "        \n",
    "        data = spikegen.rate(x,num_steps)\n",
    "\n",
    "        for q in data:\n",
    "\n",
    "            cur1 = self.fc1(q)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "        \n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0fHcAKfrav6"
   },
   "source": [
    "The code in the `forward()` function will only be called once the input argument `x` is explicitly passed into `net`.\n",
    "\n",
    "* `fc1` applies a linear transformation to all input pixels from the MNIST dataset;\n",
    "* `lif1` integrates the weighted input over time, emitting a spike if the threshold condition is met;\n",
    "* `fc2` applies a linear transformation to the output spikes of `lif1`;\n",
    "* `lif2` is another spiking neuron layer, integrating the weighted spikes over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a7MdORCtIx4"
   },
   "source": [
    "# 7. Training the SNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D-fhT3Q7nXM"
   },
   "source": [
    "## 7.1 Accuracy Metric\n",
    "Below is a function that takes a batch of data, counts up all the spikes from each neuron (i.e., a rate code over the simulation time), and compares the index of the highest count with the actual target. If they match, then the network correctly predicted the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1657784286786,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "-IxcnBAxpkoy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pass data into the network, sum the spikes over time\n",
    "# and compare the neuron with the highest number of spikes\n",
    "# with the target\n",
    "\n",
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woJSGSx68tsd"
   },
   "source": [
    "## 7.2 Loss Definition\n",
    "The `nn.CrossEntropyLoss` function in PyTorch automatically handles taking the softmax of the output layer as well as generating a loss at the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1657784286786,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "iqdVyjCNtdlp"
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1fPgSoO9Jgb"
   },
   "source": [
    "## 7.3 Optimizer\n",
    "Adam is a robust optimizer that performs well on recurrent networks, so let's use that with a learning rate of $5\\times10^{-4}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1657784286786,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "l62ZR51s9Lxg"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiqAVKzVbfPn"
   },
   "source": [
    "## 7.4 One Iteration of Training\n",
    "Take the first batch of data and load it onto CUDA if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1657784286786,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "Hv1q2-Mt9kVi"
   },
   "outputs": [],
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "data = data.to(device)\n",
    "targets = targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(batch_size, -1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFahTbAv-Vtt"
   },
   "source": [
    "Flatten the input data to a vector of size $784$ and pass it into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1657784286936,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "lltqTEXE92V-",
    "outputId": "2cd149d2-75c1-4409-9194-d69aad2072f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 128, 10])\n"
     ]
    }
   ],
   "source": [
    "spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "print(mem_rec.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wd_qv6xD-lCb"
   },
   "source": [
    "The recording of the membrane potential is taken across:\n",
    "* 25 time steps\n",
    "* 128 samples of data\n",
    "* 10 output neurons\n",
    "\n",
    "We wish to calculate the loss at every time step, and sum these up together, as per Equation $(10)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1657784286936,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "nsnH8y5G-D-z",
    "outputId": "daa0e6c2-8300-4280-98c6-2cda08cfc39a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 64962.070\n"
     ]
    }
   ],
   "source": [
    "# initialize the total loss value\n",
    "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "# sum loss at every step\n",
    "for step in range(num_steps):\n",
    "  loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "print(f\"Training loss: {loss_val.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4r0sKMV_4ri"
   },
   "source": [
    "The loss is quite large, because it is summed over 25 time steps. The accuracy is also bad (it should be roughly around 10%) as the network is untrained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1657784286937,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "qetPvz7mAArd",
    "outputId": "683f17c7-f978-469c-c2dd-2e5b69dbc7d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy for a single minibatch: 96.88%\n"
     ]
    }
   ],
   "source": [
    "print_batch_accuracy(data, targets, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUcR0GcUAtPn"
   },
   "source": [
    "A single weight update is applied to the network as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1657784287094,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "WxyBhsmlAsWM"
   },
   "outputs": [],
   "source": [
    "# clear previously stored gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# calculate the gradients\n",
    "loss_val.backward()\n",
    "\n",
    "# weight update\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubyude8eA5p9"
   },
   "source": [
    "Now, re-run the loss calculation and accuracy after a single iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1657784287095,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "l4ZquRR9A9He",
    "outputId": "0be77c82-a94c-4190-f47c-5d437209cb12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 55052.602\n",
      "Train set accuracy for a single minibatch: 96.09%\n"
     ]
    }
   ],
   "source": [
    "# calculate new network outputs using the same data\n",
    "spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "# initialize the total loss value\n",
    "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "# sum loss at every step\n",
    "for step in range(num_steps):\n",
    "  loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "print(f\"Training loss: {loss_val.item():.3f}\")\n",
    "print_batch_accuracy(data, targets, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbNPCNTSBaW3"
   },
   "source": [
    "After only one iteration, the loss should have decreased and accuracy should have increased. Note how membrane potential is used to calculate the cross entropy\n",
    "loss, and spike count is used for the measure of accuracy. It is also possible to use the spike count in the loss ([see Tutorial 6](https://snntorch.readthedocs.io/en/latest/tutorials/index.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVgKDes8BiXq"
   },
   "source": [
    "## 7.5 Training Loop\n",
    "\n",
    "Let's combine everything into a training loop. We will train for one epoch (though feel free to increase `num_epochs`), exposing our network to each sample of data once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55631,
     "status": "ok",
     "timestamp": 1657784342721,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "LMZMxEV8dcTC",
    "outputId": "aa57c896-afa2-47e6-d358-e185bd287b4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0\n",
      "Train Set Loss: 54.35\n",
      "Test Set Loss: 48.83\n",
      "Train set accuracy for a single minibatch: 57.03%\n",
      "Test set accuracy for a single minibatch: 46.09%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 50\n",
      "Train Set Loss: 14.71\n",
      "Test Set Loss: 17.26\n",
      "Train set accuracy for a single minibatch: 89.84%\n",
      "Test set accuracy for a single minibatch: 87.50%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 100\n",
      "Train Set Loss: 13.15\n",
      "Test Set Loss: 14.84\n",
      "Train set accuracy for a single minibatch: 89.06%\n",
      "Test set accuracy for a single minibatch: 91.41%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 150\n",
      "Train Set Loss: 9.71\n",
      "Test Set Loss: 8.20\n",
      "Train set accuracy for a single minibatch: 93.75%\n",
      "Test set accuracy for a single minibatch: 95.31%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 200\n",
      "Train Set Loss: 10.49\n",
      "Test Set Loss: 6.80\n",
      "Train set accuracy for a single minibatch: 88.28%\n",
      "Test set accuracy for a single minibatch: 91.41%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 250\n",
      "Train Set Loss: 8.08\n",
      "Test Set Loss: 8.28\n",
      "Train set accuracy for a single minibatch: 92.19%\n",
      "Test set accuracy for a single minibatch: 90.62%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 300\n",
      "Train Set Loss: 6.82\n",
      "Test Set Loss: 8.34\n",
      "Train set accuracy for a single minibatch: 92.97%\n",
      "Test set accuracy for a single minibatch: 93.75%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 350\n",
      "Train Set Loss: 8.32\n",
      "Test Set Loss: 10.46\n",
      "Train set accuracy for a single minibatch: 94.53%\n",
      "Test set accuracy for a single minibatch: 92.97%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 400\n",
      "Train Set Loss: 9.02\n",
      "Test Set Loss: 3.69\n",
      "Train set accuracy for a single minibatch: 92.19%\n",
      "Test set accuracy for a single minibatch: 94.53%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 450\n",
      "Train Set Loss: 3.95\n",
      "Test Set Loss: 6.11\n",
      "Train set accuracy for a single minibatch: 97.66%\n",
      "Test set accuracy for a single minibatch: 94.53%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(num_steps):\n",
    "            loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(num_steps):\n",
    "                test_loss += loss(test_mem[step], test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                train_printer()\n",
    "            counter += 1\n",
    "            iter_counter +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Taf6WZLojHTz"
   },
   "source": [
    "If this was your first time training an SNN, then congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1657784640152,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "eg8L55PelXK1",
    "outputId": "4b2704e1-c5b5-4a83-b569-4d2f045e76df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784)\n",
      "(10, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(list(net.fc1.parameters())[0].detach().cpu().numpy().shape)\n",
    "print(list(net.fc2.parameters())[0].detach().cpu().numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_131391/3881061420.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(torch.max(net.fc1.weight)/32767).to(device), #scale\n"
     ]
    }
   ],
   "source": [
    "\n",
    "layer1 = torch.quantize_per_tensor(\n",
    "    net.fc1.weight,  # the original model\n",
    "    torch.tensor(torch.max(net.fc1.weight)/32767).to(device), #scale\n",
    "    torch.tensor(0).to(device), #zero_point\n",
    "    dtype=torch.qint8)  # the target dtype for quantized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_quantization(b):\n",
    "\n",
    "    def uniform_quant(x, b):\n",
    "        xdiv = x.mul((2 ** b - 1))\n",
    "        xhard = xdiv.round().div(2 ** b - 1)\n",
    "        #print('uniform quant bit: ', b)\n",
    "        return xhard\n",
    "\n",
    "    class _pq(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, alpha):\n",
    "            input.div_(alpha)                          # weights are first divided by alpha\n",
    "            input_c = input.clamp(min=-1, max=1)       # then clipped to [-1,1]\n",
    "            sign = input_c.sign()\n",
    "            input_abs = input_c.abs()\n",
    "            input_q = uniform_quant(input_abs, b).mul(sign)\n",
    "            ctx.save_for_backward(input, input_q)\n",
    "            input_q = input_q.mul(alpha)               # rescale to the original range\n",
    "            return input_q\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            grad_input = grad_output.clone()             # grad for weights will not be clipped\n",
    "            input, input_q = ctx.saved_tensors\n",
    "            i = (input.abs()>1.).float()     # >1 means clipped. # output matrix is a form of [True, False, True, ...]\n",
    "            sign = input.sign()              # output matrix is a form of [+1, -1, -1, +1, ...]\n",
    "            #grad_alpha = (grad_output*(sign*i + (input_q-input)*(1-i))).sum()\n",
    "            grad_alpha = (grad_output*(sign*i + (0.0)*(1-i))).sum()\n",
    "            # above line, if i = True,  and sign = +1, \"grad_alpha = grad_output * 1\"\n",
    "            #             if i = False, \"grad_alpha = grad_output * (input_q-input)\"\n",
    "            grad_input = grad_input*(1-i)\n",
    "            return grad_input, grad_alpha\n",
    "\n",
    "    return _pq().apply\n",
    "\n",
    "class weight_quantize_fn(nn.Module):\n",
    "    def __init__(self, w_bit):\n",
    "        super(weight_quantize_fn, self).__init__()\n",
    "        self.w_bit = w_bit-1\n",
    "        self.weight_q = weight_quantization(b=self.w_bit)\n",
    "        self.register_parameter('wgt_alpha', Parameter(torch.tensor(3.0)))\n",
    "\n",
    "    def forward(self, weight):\n",
    "        mean = weight.data.mean()\n",
    "        std = weight.data.std()\n",
    "        weight = weight.add(-mean).div(std)      # weights normalization\n",
    "        weight_q = self.weight_q(weight, self.wgt_alpha)\n",
    "\n",
    "        return weight_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=1000, bias=True)\n",
       "  (lif1): Leaky()\n",
       "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
       "  (lif2): Leaky()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path = '/Volumes/export/isn/gopa/CRI_proj/L2S_justin/snntorch_mnist/result/mnist_2layer_MLP_quantized/model_best.pth.tar'\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model_best = net\n",
    "model_best.load_state_dict(checkpoint['state_dict'])\n",
    "model_best.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Results\n",
    "## 8.1 Plot Training/Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1657784342991,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "_Pk_EScnpkpj",
    "outputId": "a436a403-17e9-43cc-e7f0-ef8bcc63d76f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot Loss\u001b[39;00m\n\u001b[1;32m      2\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mloss_hist\u001b[49m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(test_loss_hist)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss Curves\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_hist' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-Gd84OAl1rB"
   },
   "source": [
    "The loss curves are noisy because the losses are tracked at every iteration, rather than averaging across multiple iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3f0vBnBpkpk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 8.2 Test Set Accuracy\n",
    "This function iterates over all minibatches to obtain a measure of accuracy over the full 10,000 samples in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3948,
     "status": "ok",
     "timestamp": 1657785022241,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "F5Rb4xHGndQh",
    "outputId": "e7feaf22-89b0-4c9a-c8cb-bc5faea77f42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correctly classified test set images: 9649/10000\n",
      "Test Set Accuracy: 96.49%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# drop_last switched to False to keep all samples\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "  net.eval()\n",
    "  for data, targets in test_loader:\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    # forward pass\n",
    "    test_spk, _ = net(data.view(data.size(0), -1))\n",
    "\n",
    "    # calculate total accuracy\n",
    "    _, predicted = test_spk.sum(dim=0).max(1)\n",
    "    total += targets.size(0)\n",
    "    correct += (predicted == targets).sum().item()\n",
    "\n",
    "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
    "print(f\"Test Set Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBIXau4Zpkpl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Voila! That's it for static MNIST. Feel free to tweak the network parameters, hyperparameters, decay rate, using a learning rate scheduler etc. to see if you can improve the network performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map trained SNN from torchsnn to CRI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 511,
     "status": "ok",
     "timestamp": 1657784951693,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "i2X967KEplnf",
    "outputId": "be8ca087-c79a-4317-c0ba-ec341975d288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4160.0\n",
      "3713.0\n"
     ]
    }
   ],
   "source": [
    "# extract weights and bias for torchsnn\n",
    "layers = [list(model_best.fc1.parameters())[0].detach().cpu().numpy(), list(model_best.fc2.parameters())[0].detach().cpu().numpy()]\n",
    "biases = [model_best.fc1.bias.detach().cpu().numpy(), model_best.fc2.bias.detach().cpu().numpy()]\n",
    "\n",
    "print(np.min(layers[1]))\n",
    "print(np.max(layers[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights shape:  (1000, 784)\n",
      "constructing Axons\n",
      "Input layer shape(outfeature, infeature):  (1000, 784)\n",
      "axon offset:  784\n",
      "Construct bias axons for hidden layers: (1000,)\n",
      "Weights shape:  (10, 1000)\n",
      "constructing output layer\n",
      "output layer shape(outfeature, infeature):  (10, 1000)\n",
      "instantiate output neurons\n",
      "Construct bias axons for output neurons (10,)\n",
      "output neurons:  ['1000', '1001', '1002', '1003', '1004', '1005', '1006', '1007', '1008', '1009']\n",
      "number of axons:  1794\n"
     ]
    }
   ],
   "source": [
    "# scale = 9*10**4\n",
    "axonsDict = {}\n",
    "neuronsDict = {}\n",
    "outputs = []\n",
    "bias_axon = {}\n",
    "\n",
    "axonOffset = 0\n",
    "currLayerNeuronIdxOffset = 0\n",
    "nextLayerNeuronIdxOffset = 0\n",
    "for layerNum, layer in enumerate(layers):\n",
    "    inFeatures = layer.shape[1]\n",
    "    outFeatures = layer.shape[0]\n",
    "    shape = layer.shape\n",
    "    weight = layer\n",
    "    bias = biases[layerNum]\n",
    "    print(\"Weights shape: \", np.shape(weight))\n",
    "    if (layerNum == 0):\n",
    "        print('constructing Axons')\n",
    "        print(\"Input layer shape(outfeature, infeature): \", weight.shape)\n",
    "        for axonIdx, axon in enumerate(weight.T):\n",
    "            #print(axonIdx)\n",
    "            axonID = 'a'+str(axonIdx)\n",
    "            axonEntry = [(str(postSynapticID), int(synapseWeight)) for postSynapticID, synapseWeight in enumerate(axon) ]\n",
    "            axonsDict[axonID] = axonEntry\n",
    "        axonOffset += inFeatures\n",
    "        print(\"axon offset: \",axonOffset)\n",
    "        #implmenting bias: for each bias add a axon with corresponding weights with synapse (neuron, bias_val)\n",
    "        print('Construct bias axons for hidden layers:',bias.shape)\n",
    "        for neuronIdx, bias_value in enumerate(bias):\n",
    "            biasAxonID = 'a'+str(neuronIdx + axonOffset)\n",
    "            biasAxonEntry = [(str(neuronIdx),int(bias_value))]\n",
    "            axonsDict[biasAxonID] = biasAxonEntry\n",
    "        \n",
    "    elif (layerNum == len(layers)-1):\n",
    "        print('constructing output layer')\n",
    "        nextLayerNeuronIdxOffset += inFeatures\n",
    "        print(\"output layer shape(outfeature, infeature): \", weight.shape)\n",
    "        for baseNeuronIdx, neuron in enumerate(weight.T):\n",
    "            neuronID = str(baseNeuronIdx+currLayerNeuronIdxOffset)\n",
    "            neuronEntry = [(str(basePostSynapticID+nextLayerNeuronIdxOffset), int(synapseWeight)) for basePostSynapticID, synapseWeight in enumerate(neuron) if synapseWeight != 0]\n",
    "            neuronsDict[neuronID] = neuronEntry\n",
    "            #print(neuronID)\n",
    "        currLayerNeuronIdxOffset += inFeatures\n",
    "        #instantiate the output neurons\n",
    "        print('instantiate output neurons')\n",
    "        for baseNeuronIdx in range(outFeatures):\n",
    "            neuronID = str(baseNeuronIdx+nextLayerNeuronIdxOffset)\n",
    "            neuronsDict[neuronID] = []\n",
    "            outputs.append(neuronID)\n",
    "            #print(neuronID)\n",
    "        #implmenting bias: for each bias add a axon with corresponding weights with synapse (neuron, bias_val)\n",
    "        print('Construct bias axons for output neurons',bias.shape)\n",
    "        axonOffset += inFeatures\n",
    "        for neuronIdx, bias_value in enumerate(bias):\n",
    "            biasAxonID = 'a'+str(neuronIdx + axonOffset)\n",
    "            biasAxonEntry = [(str(neuronIdx+nextLayerNeuronIdxOffset),int(bias_value))]\n",
    "            axonsDict[biasAxonID] = biasAxonEntry\n",
    "            \n",
    "    else:\n",
    "        print('constructing hidden layer')\n",
    "        nextLayerNeuronIdxOffset += inFeatures\n",
    "        for baseNeuronIdx, neuron in enumerate(weight): #SHOULD THIS BE A TRANSPOSE\n",
    "            neuronID = str(baseNeuronIdx+currLayerNeuronIdxOffset)\n",
    "            neuronEntry = [(str(basePostSynapticID+nextLayerNeuronIdxOffset), int(synapseWeight)) for basePostSynapticID, synapseWeight in enumerate(neuron) if synapseWeight != 0 ]\n",
    "            neuronsDict[neuronID] = neuronEntry\n",
    "            #print(neuronID)\n",
    "        currLayerNeuronIdxOffset += inFeatures\n",
    "        axonOffset += inFeatures\n",
    "        for neuronIdx, bias_value in enumerate(bias):\n",
    "            biasAxonID = 'a'+str(neuronIdx + axonOffset)\n",
    "            biasAxonEntry = [(str(neuronIdx+nextLayerNeuronIdxOffset),int(bias_value))]\n",
    "            axonsDict[biasAxonID] = biasAxonEntry\n",
    "print(\"output neurons: \", outputs)\n",
    "print(\"number of axons: \", len(axonsDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of axons:  1794\n",
      "Total number of connections between axon and neuron:  785010\n",
      "Max fan out of axon:  1000\n",
      "---\n",
      "Number of neurons:  1010\n",
      "Total number of connections between hidden and output layers:  9996\n",
      "Max fan out of neuron:  10\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of axons: \",len(axonsDict))\n",
    "totalAxonSyn = 0\n",
    "maxFan = 0\n",
    "for key in axonsDict.keys():\n",
    "    totalAxonSyn += len(axonsDict[key])\n",
    "    if len(axonsDict[key]) > maxFan:\n",
    "        maxFan = len(axonsDict[key])\n",
    "print(\"Total number of connections between axon and neuron: \", totalAxonSyn)\n",
    "print(\"Max fan out of axon: \", maxFan)\n",
    "print('---')\n",
    "print(\"Number of neurons: \", len(neuronsDict))\n",
    "totalSyn = 0\n",
    "maxFan = 0\n",
    "for key in neuronsDict.keys():\n",
    "    totalSyn += len(neuronsDict[key])\n",
    "    if len(neuronsDict[key]) > maxFan:\n",
    "        maxFan = len(neuronsDict[key])\n",
    "print(\"Total number of connections between hidden and output layers: \", totalSyn)\n",
    "print(\"Max fan out of neuron: \", maxFan)\n",
    "print(neuronsDict['1007'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added axons to connectome\n",
      "added neurons to connectome\n",
      "added axon synpases\n",
      "added neuron synapses\n",
      "generated Connectome\n",
      "initialized\n",
      "added axons to connectome\n",
      "added neurons to connectome\n",
      "added axon synpases\n",
      "added neuron synapses\n",
      "generated Connectome\n"
     ]
    }
   ],
   "source": [
    "from l2s.api import CRI_network\n",
    "config = {}\n",
    "config['neuron_type'] = \"I&F\"\n",
    "config['global_neuron_params'] = {}\n",
    "config['global_neuron_params']['v_thr'] = 9*10**4\n",
    "softwareNetwork = CRI_network(axons=axonsDict,connections=neuronsDict,config=config,target='simpleSim', outputs = outputs)\n",
    "hardwareNetwork = CRI_network(axons=axonsDict,connections=neuronsDict,config=config,target='CRI', outputs = outputs,simDump = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1657785754863,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "f_oW9WpWr9Sd"
   },
   "outputs": [],
   "source": [
    "def input_to_CRI(currentInput):\n",
    "    num_steps = 10\n",
    "    currentInput = data.view(data.size(0), -1)\n",
    "    batch = []\n",
    "    n = 0\n",
    "    for element in currentInput:\n",
    "        timesteps = []\n",
    "        rateEnc = spikegen.rate(element,num_steps)\n",
    "        rateEnc = rateEnc.detach().cpu().numpy()\n",
    "        for element in rateEnc:\n",
    "            currInput = ['a'+str(idx) for idx,axon in enumerate(element) if axon != 0]\n",
    "            biasInput = ['a'+str(idx) for idx in range(784,len(axonsDict))]\n",
    "#             timesteps.append(currInput)\n",
    "#             timesteps.append(biasInput)\n",
    "            timesteps.append(currInput+biasInput)\n",
    "        batch.append(timesteps)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CRI(inputList, output_offset):\n",
    "    predictions = []\n",
    "    total_time_cri = 0\n",
    "    #each image\n",
    "    for currInput in inputList:\n",
    "        #reset the membrane potential to zero\n",
    "        softwareNetwork.simpleSim.initialize_sim_vars(len(neuronsDict))\n",
    "        spikeRate = [0]*10\n",
    "        #each time step\n",
    "        for slice in currInput:\n",
    "            start_time = time.time()\n",
    "            swSpike = softwareNetwork.step(slice, membranePotential=False)\n",
    "            end_time = time.time()\n",
    "            total_time_cri = total_time_cri + end_time-start_time\n",
    "            for spike in swSpike:\n",
    "                spikeIdx = int(spike) - output_offset \n",
    "                try: \n",
    "                    if spikeIdx >= 0: \n",
    "                        spikeRate[spikeIdx] += 1 \n",
    "                except:\n",
    "                    print(\"SpikeIdx: \", spikeIdx,\"\\n SpikeRate:\",spikeRate )\n",
    "        predictions.append(spikeRate.index(max(spikeRate)))\n",
    "    print(f\"Total simulation execution time: {total_time_cri:.5f} s\")\n",
    "\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CRI_hw(inputList,output_offset):\n",
    "    predictions = []\n",
    "    #each image\n",
    "    total_time_cri = 0\n",
    "    for currInput in inputList:\n",
    "        #initiate the softwareNetwork for each image\n",
    "        cri_simulations.FPGA_Execution.fpga_controller.clear(len(neuronsDict), False, 0)  ##Num_neurons, simDump, coreOverride\n",
    "        spikeRate = [0]*10\n",
    "        #each time step\n",
    "        for slice in currInput:\n",
    "            start_time = time.time()\n",
    "            hwSpike = hardwareNetwork.step(slice, membranePotential=False)\n",
    "#             print(\"Mem:\",mem)\n",
    "            end_time = time.time()\n",
    "            total_time_cri = total_time_cri + end_time-start_time\n",
    "            print(hwSpike)\n",
    "            for spike in hwSpike:\n",
    "                print(int(spike))\n",
    "                spikeIdx = int(spike) - output_offset \n",
    "                if spikeIdx >= 0: \n",
    "                    spikeRate[spikeIdx] += 1 \n",
    "        predictions.append(spikeRate.index(max(spikeRate))) \n",
    "    print(f\"Total execution time CRIFPGA: {total_time_cri:.5f} s\")\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# pickle.dump( axonsDict, open( \"axonsDict.p\", \"wb\" ) )\n",
    "# pickle.dump( neuronsDict, open( \"neuronsDict.p\", \"wb\" ) )# pickle.dump( outputs, open( \"outputs.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRI Predicted:  tensor([9, 3, 5, 7, 2, 3, 3, 4, 1, 3, 2, 1, 7, 0, 1, 6, 2, 9, 1, 3, 0, 6, 5, 1,\n",
      "        5, 9, 1, 1, 8, 1, 2, 9, 5, 4, 2, 1, 0, 7, 2, 9, 8, 8, 6, 1, 3, 0, 0, 9,\n",
      "        0, 9, 1, 4, 3, 1, 7, 7, 6, 5, 8, 8, 7, 7, 0, 3, 6, 1, 1, 4, 3, 3, 8, 6,\n",
      "        8, 0, 8, 2, 6, 6, 4, 7, 1, 1, 3, 3, 3, 4, 2, 4, 6, 9, 3, 5, 0, 4, 5, 4,\n",
      "        2, 1, 8, 5, 0, 4, 0, 1, 7, 0, 1, 8, 0, 5, 9, 0, 1, 2, 0, 8, 1, 1, 9, 8,\n",
      "        4, 7, 5, 9, 0, 9, 7, 0], device='cuda:0')\n",
      "Target:  tensor([9, 3, 5, 7, 2, 3, 3, 4, 1, 3, 2, 1, 7, 0, 1, 6, 2, 9, 1, 3, 0, 5, 5, 1,\n",
      "        5, 9, 1, 1, 8, 1, 2, 9, 5, 4, 2, 1, 3, 7, 2, 9, 8, 8, 6, 1, 3, 0, 0, 9,\n",
      "        0, 8, 1, 4, 3, 1, 7, 7, 6, 6, 8, 8, 7, 7, 0, 3, 6, 1, 1, 4, 3, 3, 8, 6,\n",
      "        8, 0, 3, 2, 6, 6, 4, 7, 1, 1, 3, 3, 3, 4, 2, 4, 6, 9, 3, 5, 0, 4, 5, 4,\n",
      "        2, 1, 8, 5, 0, 4, 0, 1, 7, 4, 1, 8, 4, 5, 9, 0, 1, 2, 3, 8, 1, 1, 9, 8,\n",
      "        4, 7, 5, 9, 0, 9, 7, 0], device='cuda:0')\n",
      "Torchsnn Predicted:  tensor([9, 3, 5, 7, 2, 3, 3, 4, 1, 3, 2, 1, 7, 0, 1, 6, 2, 9, 1, 3, 0, 6, 5, 1,\n",
      "        5, 9, 1, 1, 8, 1, 2, 9, 5, 4, 2, 1, 3, 7, 2, 9, 8, 8, 6, 1, 3, 0, 0, 9,\n",
      "        0, 9, 1, 4, 3, 1, 7, 7, 6, 5, 8, 8, 7, 7, 0, 3, 6, 1, 1, 4, 3, 3, 8, 6,\n",
      "        8, 0, 3, 2, 6, 6, 4, 7, 1, 1, 3, 3, 3, 4, 2, 4, 6, 9, 3, 5, 0, 0, 5, 4,\n",
      "        2, 1, 8, 5, 0, 4, 0, 1, 7, 8, 1, 8, 4, 5, 9, 0, 1, 2, 0, 8, 1, 1, 9, 8,\n",
      "        4, 7, 5, 9, 0, 9, 7, 0], device='cuda:0')\n",
      "Totoal execution time: 411.73 s\n",
      "Total correctly classified test set images for TorchSNN: 122/128\n",
      "Total correctly classified test set images for CRI: 120/128\n",
      "Test Set Accuracy for TorchSNN: 95.31%\n",
      "Test Set Accuracy for CRI: 93.75%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "cri_correct = 0\n",
    "# drop_last switched to False to keep all samples\n",
    "test_loader = DataLoader(mnist_test, batch_size=128, shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for data, targets in test_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        input = input_to_CRI(data)\n",
    "        criPred = torch.tensor(run_CRI(input)).to(device)\n",
    "        print(\"CRI Predicted: \",criPred)\n",
    "        criPred_hw = torch.tensor(run_CRI_hw(input,output_offset)).to(device)\n",
    "        print(\"CRI Predicted HW: \",criPred_hw)\n",
    "        # print(data.shape)\n",
    "        # forward pass\n",
    "        test_spk, _ = net(data.view(data.size(0), -1))\n",
    "\n",
    "        # calculate total accuracy\n",
    "        _, predicted = test_spk.sum(dim=0).max(1)\n",
    "        print(\"Torchsnn Predicted: \",predicted)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        cri_correct += (criPred == targets).sum().item()\n",
    "        cri_correct_hw += (criPred_hw == targets).sum().item()\n",
    "        break #run for one batch\n",
    "\n",
    "print(f\"Total correctly classified test set images for TorchSNN: {correct}/{total}\")\n",
    "print(f\"Total correctly classified test set images for CRI: {cri_correct}/{total}\")\n",
    "print(f\"Test Set Accuracy for TorchSNN: {100 * correct / total:.2f}%\")\n",
    "print(f\"Test Set Accuracy for CRI: {100 * cri_correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hardwareNetwork.sim_flush('/home/justinData/jul15Dump.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0dAgWUt2o6E"
   },
   "source": [
    "# Conclusion\n",
    "Now you know how to construct and train a fully-connected network on a static dataset. The spiking neurons can also be adapted to other layer types, including convolutions and skip connections. Armed with this knowledge, you should now be able to build many different types of SNNs. [In the next tutorial](https://snntorch.readthedocs.io/en/latest/tutorials/index.html), you will learn how to train a spiking convolutional network, and simplify the amount of code required using the `snn.backprop` module.\n",
    "\n",
    "Also, a special thanks to Bugra Kaytanli for providing valuable feedback on the tutorial.\n",
    "\n",
    "If you like this project, please consider starring ⭐ the repo on GitHub as it is the easiest and best way to support it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiKZzTUTkcjw"
   },
   "source": [
    "# Additional Resources\n",
    "\n",
    "* [Check out the snnTorch GitHub project here.](https://github.com/jeshraghian/snntorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try matching a tiny SNNTorch network with the output of CRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1657784286599,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "lI0GbgLgpkos",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataloader arguments\n",
    "batch_size = 258\n",
    "data_path='~/justinData/mnistData'\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1657784286599,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "2fhRixcspkot",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((5,1)), #make the image drastically smaller\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1657784286600,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "aEtCbO6upkou",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1657784286784,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "Lud3kywn55fj"
   },
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = 5\n",
    "num_hidden = 10\n",
    "num_outputs = 10\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 25\n",
    "beta = 0\n",
    "from snntorch import spikeplot as splt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1657784286785,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "-uquHLLmpkox",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        #self.fc1 = nn.Linear(num_inputs, num_hidden, bias=False)\n",
    "        #self.lif1 = snn.Leaky(beta=beta,reset_mechanism='zero')\n",
    "        \n",
    "        #self.fc2 = nn.Linear(num_hidden, num_outputs, bias=False)\n",
    "        #self.lif2 = snn.Leaky(beta=beta,reset_mechanism='zero')\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden, bias=False)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        \n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs, bias=False)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        spk1_rec = []\n",
    "        mem2_rec = []\n",
    "        \n",
    "        #data = spikegen.rate(x,num_steps)\n",
    "\n",
    "        #for q in data:\n",
    "        #    cur1 = self.fc1(q)\n",
    "        #    spk1, mem1 = self.lif1(cur1, mem1)\n",
    "        #    cur2 = self.fc2(spk1)\n",
    "        #    spk2, mem2 = self.lif2(cur2, mem2)\n",
    "        #    spk2_rec.append(spk2)\n",
    "        #    mem2_rec.append(mem2)\n",
    "\n",
    "        #return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "        # network simulation\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step]) # post-synaptic current <-- spk_in x weight\n",
    "            spk1, mem1 = self.lif1(cur1, mem1) # mem[t+1] <--post-syn current + decayed membrane\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            mem2_rec.append(mem2)\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "\n",
    "        # convert lists to tensors\n",
    "        mem2_rec = torch.stack(mem2_rec)\n",
    "        spk1_rec = torch.stack(spk1_rec)\n",
    "        spk2_rec = torch.stack(spk2_rec)\n",
    "        \n",
    "        return spk2_rec\n",
    "\n",
    "       \n",
    "        \n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_in = spikegen.rate_conv(torch.rand((200, num_inputs))).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(spk_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [list(net.fc1.parameters())[0].detach().cpu().numpy(), list(net.fc2.parameters())[0].detach().cpu().numpy()]\n",
    "print(layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define lossfunction and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1657784286786,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "-IxcnBAxpkoy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pass data into the network, sum the spikes over time\n",
    "# and compare the neuron with the highest number of spikes\n",
    "# with the target\n",
    "\n",
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1657784286786,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "iqdVyjCNtdlp"
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1657784286786,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "l62ZR51s9Lxg"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin Training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55631,
     "status": "ok",
     "timestamp": 1657784342721,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "LMZMxEV8dcTC",
    "outputId": "aa57c896-afa2-47e6-d358-e185bd287b4e"
   },
   "source": [
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        #print(data.view(batch_size, -1).shape)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(num_steps):\n",
    "            loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(num_steps):\n",
    "                test_loss += loss(test_mem[step], test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                train_printer()\n",
    "            counter += 1\n",
    "            iter_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3948,
     "status": "ok",
     "timestamp": 1657785022241,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "F5Rb4xHGndQh",
    "outputId": "e7feaf22-89b0-4c9a-c8cb-bc5faea77f42"
   },
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# drop_last switched to False to keep all samples\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "  net.eval()\n",
    "  for data, targets in test_loader:\n",
    "    print(data.shape)\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    # forward pass\n",
    "    test_spk, _ = net(data.view(data.size(0), -1))\n",
    "    print(test_spk.shape)\n",
    "    break\n",
    "\"\"\"\n",
    "    # calculate total accuracy\n",
    "    _, predicted = test_spk.sum(dim=0).max(1)\n",
    "    total += targets.size(0)\n",
    "    correct += (predicted == targets).sum().item()\n",
    "\n",
    "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
    "print(f\"Test Set Accuracy: {100 * correct / total:.2f}%\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of Copy of tutorial_2_FCN_truncatedfromscratch.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/jeshraghian/snntorch/blob/master/examples/tutorial_5_FCN.ipynb",
     "timestamp": 1657785814184
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "782c4fc05a7b0c5006502edc276c124083adbfff5066531c0f613c007bf9a5ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
