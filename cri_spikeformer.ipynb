{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e773c004-a9c7-458c-aaca-3bf43e147ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn.parameter import Parameter\n",
    "from spikingjelly.clock_driven.neuron import MultiStepLIFNode\n",
    "from timm.models.layers import to_2tuple, trunc_normal_, DropPath\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.vision_transformer import _cfg\n",
    "from timm.utils import *\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm, trange\n",
    "import copy\n",
    "\n",
    "\n",
    "#TODO: Define the model the spikeformer model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1_linear = nn.Linear(in_features, hidden_features)\n",
    "        self.fc1_bn = nn.BatchNorm1d(hidden_features)\n",
    "        self.fc1_lif = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='torch')\n",
    "\n",
    "        self.fc2_linear = nn.Linear(hidden_features, out_features)\n",
    "        self.fc2_bn = nn.BatchNorm1d(out_features)\n",
    "        self.fc2_lif = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='torch')\n",
    "\n",
    "        self.c_hidden = hidden_features\n",
    "        self.c_output = out_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        T,B,N,C = x.shape\n",
    "        x_ = x.flatten(0, 1)\n",
    "        x = self.fc1_linear(x_)\n",
    "        x = self.fc1_bn(x.transpose(-1, -2)).transpose(-1, -2).reshape(T, B, N, self.c_hidden).contiguous()\n",
    "        x = self.fc1_lif(x)\n",
    "\n",
    "        x = self.fc2_linear(x.flatten(0,1))\n",
    "        x = self.fc2_bn(x.transpose(-1, -2)).transpose(-1, -2).reshape(T, B, N, C).contiguous()\n",
    "        x = self.fc2_lif(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SSA(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = 0.125\n",
    "        self.q_linear = nn.Linear(dim, dim)\n",
    "        self.q_bn = nn.BatchNorm1d(dim)\n",
    "        self.q_lif = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='torch')\n",
    "\n",
    "        self.k_linear = nn.Linear(dim, dim)\n",
    "        self.k_bn = nn.BatchNorm1d(dim)\n",
    "        self.k_lif = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='torch')\n",
    "\n",
    "        self.v_linear = nn.Linear(dim, dim)\n",
    "        self.v_bn = nn.BatchNorm1d(dim)\n",
    "        self.v_lif = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='torch')\n",
    "        self.attn_lif = MultiStepLIFNode(tau=2.0, v_threshold=0.5, detach_reset=True, backend='torch')\n",
    "\n",
    "        self.proj_linear = nn.Linear(dim, dim)\n",
    "        self.proj_bn = nn.BatchNorm1d(dim)\n",
    "        self.proj_lif = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='torch')\n",
    "\n",
    "    def forward(self, x):\n",
    "        T,B,N,C = x.shape\n",
    "\n",
    "        x_for_qkv = x.flatten(0, 1)  # TB, N, C\n",
    "        q_linear_out = self.q_linear(x_for_qkv)  # [TB, N, C]\n",
    "        # print(\"q_linear_out: \", q_linear_out.shape)\n",
    "        q_linear_out = self.q_bn(q_linear_out. transpose(-1, -2)).transpose(-1, -2).reshape(T, B, N, C).contiguous()\n",
    "        q_linear_out = self.q_lif(q_linear_out)\n",
    "        q = q_linear_out.reshape(T, B, N, self.num_heads, C//self.num_heads).permute(0, 1, 3, 2, 4).contiguous()\n",
    "\n",
    "        k_linear_out = self.k_linear(x_for_qkv)\n",
    "        # print(\"k_linear_out after bn: \", k_linear_out.shape)\n",
    "        k_linear_out = self.k_bn(k_linear_out. transpose(-1, -2)).transpose(-1, -2).reshape(T,B,C,N).contiguous()\n",
    "        k_linear_out = self.k_lif(k_linear_out)\n",
    "        k = k_linear_out.reshape(T, B, N, self.num_heads, C//self.num_heads).permute(0, 1, 3, 2, 4).contiguous()\n",
    "\n",
    "        v_linear_out = self.v_linear(x_for_qkv)\n",
    "        # print(\"v_linear_out after bn: \", v_linear_out.shape)\n",
    "        v_linear_out = self.v_bn(v_linear_out. transpose(-1, -2)).transpose(-1, -2).reshape(T,B,C,N).contiguous()\n",
    "        v_linear_out = self.v_lif(v_linear_out)\n",
    "        v = v_linear_out.reshape(T, B, N, self.num_heads, C//self.num_heads).permute(0, 1, 3, 2, 4).contiguous()\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        x = attn @ v\n",
    "        x = x.transpose(2, 3).reshape(T, B, N, C).contiguous()\n",
    "        x = self.attn_lif(x)\n",
    "        x = x.flatten(0, 1)\n",
    "        x = self.proj_linear(x).transpose(-1, -2)\n",
    "        # print(\"proj_norm_input\", x.shape)\n",
    "        x = self.proj_bn(x)\n",
    "        x = self.proj_lif(x.transpose(-1, -2).reshape(T, B, N, C))\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, sr_ratio=1):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = SSA(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_attn = self.attn(x)\n",
    "        x = x + x_attn\n",
    "        x = x + self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SPS(nn.Module):\n",
    "    def __init__(self, img_size_h=128, img_size_w=128, patch_size=4, in_channels=2, embed_dims=256):\n",
    "        super().__init__()\n",
    "        self.image_size = [img_size_h, img_size_w]\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "        self.C = in_channels\n",
    "        self.H, self.W = self.image_size[0] // patch_size[0], self.image_size[1] // patch_size[1]\n",
    "        self.num_patches = self.H * self.W\n",
    "        self.proj_conv = nn.Conv2d(in_channels, embed_dims//8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.proj_bn = nn.BatchNorm2d(embed_dims//8)\n",
    "        self.proj_lif = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='torch')\n",
    "\n",
    "        self.proj_conv1 = nn.Conv2d(embed_dims//8, embed_dims//4, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.proj_bn1 = nn.BatchNorm2d(embed_dims//4)\n",
    "        self.proj_lif1 = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='torch')\n",
    "\n",
    "        self.proj_conv2 = nn.Conv2d(embed_dims//4, embed_dims//2, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.proj_bn2 = nn.BatchNorm2d(embed_dims//2)\n",
    "        self.proj_lif2 = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='torch')\n",
    "        self.maxpool2 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "\n",
    "        self.proj_conv3 = nn.Conv2d(embed_dims//2, embed_dims, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.proj_bn3 = nn.BatchNorm2d(embed_dims)\n",
    "        self.proj_lif3 = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='torch')\n",
    "        self.maxpool3 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "\n",
    "        self.rpe_conv = nn.Conv2d(embed_dims, embed_dims, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.rpe_bn = nn.BatchNorm2d(embed_dims)\n",
    "        self.rpe_lif = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='torch')\n",
    "\n",
    "    def forward(self, x):\n",
    "        T, B, C, H, W = x.shape\n",
    "        x = self.proj_conv(x.flatten(0, 1)) # have some fire value\n",
    "        x = self.proj_bn(x).reshape(T, B, -1, H, W).contiguous()\n",
    "        x = self.proj_lif(x).flatten(0, 1).contiguous()\n",
    "\n",
    "        x = self.proj_conv1(x)\n",
    "        x = self.proj_bn1(x).reshape(T, B, -1, H, W).contiguous()\n",
    "        x = self.proj_lif1(x).flatten(0, 1).contiguous()\n",
    "\n",
    "        x = self.proj_conv2(x)\n",
    "        x = self.proj_bn2(x).reshape(T, B, -1, H, W).contiguous()\n",
    "        x = self.proj_lif2(x).flatten(0, 1).contiguous()\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.proj_conv3(x)\n",
    "        x = self.proj_bn3(x).reshape(T, B, -1, H//2, W//2).contiguous()\n",
    "        x = self.proj_lif3(x).flatten(0, 1).contiguous()\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x_feat = x.reshape(T, B, -1, H//4, W//4).contiguous()\n",
    "        x = self.rpe_conv(x)\n",
    "        x = self.rpe_bn(x).reshape(T, B, -1, H//4, W//4).contiguous()\n",
    "        x = self.rpe_lif(x)\n",
    "        x = x + x_feat\n",
    "\n",
    "        x = x.flatten(-2).transpose(-1, -2)  # T,B,N,C\n",
    "        return x\n",
    "\n",
    "\n",
    "class Spikformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size_h=32, img_size_w=32, patch_size=4, in_channels=3, num_classes=10,\n",
    "                 embed_dims=384, num_heads=12, mlp_ratios=4, qkv_bias=False, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n",
    "                 depths=4, sr_ratios=1, T = 4\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.T = T  # time step\n",
    "        self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depths)]  # stochastic depth decay rule\n",
    "\n",
    "        patch_embed = SPS(img_size_h=img_size_h,\n",
    "                                 img_size_w=img_size_w,\n",
    "                                 patch_size=patch_size,\n",
    "                                 in_channels=in_channels,\n",
    "                                 embed_dims=embed_dims)\n",
    "\n",
    "        block = nn.ModuleList([Block(\n",
    "            dim=embed_dims, num_heads=num_heads, mlp_ratio=mlp_ratios, qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[j],\n",
    "            norm_layer=norm_layer, sr_ratio=sr_ratios)\n",
    "            for j in range(depths)])\n",
    "\n",
    "        setattr(self, f\"patch_embed\", patch_embed)\n",
    "        setattr(self, f\"block\", block)\n",
    "\n",
    "        # classification head\n",
    "        self.head = nn.Linear(embed_dims, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def _get_pos_embed(self, pos_embed, patch_embed, H, W):\n",
    "        if H * W == self.patch_embed1.num_patches:\n",
    "            return pos_embed\n",
    "        else:\n",
    "            return F.interpolate(\n",
    "                pos_embed.reshape(1, patch_embed.H, patch_embed.W, -1).permute(0, 3, 1, 2),\n",
    "                size=(H, W), mode=\"bilinear\").reshape(1, -1, H * W).permute(0, 2, 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "\n",
    "        block = getattr(self, f\"block\")\n",
    "        patch_embed = getattr(self, f\"patch_embed\")\n",
    "\n",
    "        x = patch_embed(x)\n",
    "        for blk in block:\n",
    "            x = blk(x)\n",
    "        return x.mean(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x.unsqueeze(0)).repeat(self.T, 1, 1, 1, 1)\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x.mean(0))\n",
    "        return x\n",
    "\n",
    "    \n",
    "#Import CIFAR datasets\n",
    "\n",
    "# dataloader arguments\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor()])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "CIFAR_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "CIFAR_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "classes = CIFAR_train.classes\n",
    "\n",
    "#reduce datasize\n",
    "# subsets_train = list(range(0, len(CIFAR_train), 10))\n",
    "# subsets_test = list(range(0, len(CIFAR_test), 10))\n",
    "\n",
    "# CIFAR_train = torch.utils.data.Subset(CIFAR_train, subsets_train)\n",
    "# CIFAR_test = torch.utils.data.Subset(CIFAR_test, subsets_test)\n",
    "\n",
    "train_loader = DataLoader(CIFAR_train,batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(CIFAR_test,batch_size=batch_size,shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "138cdbea-8311-4609-a137-a51cfc77f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy(Y, targets):\n",
    "#     Y = torch.argmax(Y, axis=1)\n",
    "#     total = Y.shape[0]\n",
    "#     return torch.sum(Y==targets).detach().cpu().item()/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c7c79e5-ce6f-436a-8ff1-27ced0343bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(train_loader, net):\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        acc = 0\n",
    "        net.eval()\n",
    "        \n",
    "        train_loader = iter(train_loader)\n",
    "        for batch in tqdm(train_loader):\n",
    "            data, targets = batch\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            out = net(data)\n",
    "            acc1, acc5 = accuracy(out, targets, topk=(1,5))\n",
    "            \n",
    "    return acc1, acc5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1fa0937-3972-42f0-86bf-08f34ba611c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained model \n",
    "net = Spikformer()\n",
    "checkpoint_path = \"/Volumes/export/isn/gopa/CRI_proj/cri_spikeformer/cifar10/output/train/20230301-194515-spikformer-32/checkpoint-307.pth.tar\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "net.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f4acc8-6788-4318-913b-01d6d2b924c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     net.cuda()\n",
    "# batch_accuracy(test_loader,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32dbe3d4-a6d4-46fb-94ed-01884ab992e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─SPS: 1-1                                    --\n",
      "|    └─Conv2d: 2-1                            1,296\n",
      "|    └─BatchNorm2d: 2-2                       96\n",
      "|    └─MultiStepLIFNode: 2-3                  --\n",
      "|    |    └─Sigmoid: 3-1                      --\n",
      "|    └─Conv2d: 2-4                            41,472\n",
      "|    └─BatchNorm2d: 2-5                       192\n",
      "|    └─MultiStepLIFNode: 2-6                  --\n",
      "|    |    └─Sigmoid: 3-2                      --\n",
      "|    └─Conv2d: 2-7                            165,888\n",
      "|    └─BatchNorm2d: 2-8                       384\n",
      "|    └─MultiStepLIFNode: 2-9                  --\n",
      "|    |    └─Sigmoid: 3-3                      --\n",
      "|    └─MaxPool2d: 2-10                        --\n",
      "|    └─Conv2d: 2-11                           663,552\n",
      "|    └─BatchNorm2d: 2-12                      768\n",
      "|    └─MultiStepLIFNode: 2-13                 --\n",
      "|    |    └─Sigmoid: 3-4                      --\n",
      "|    └─MaxPool2d: 2-14                        --\n",
      "|    └─Conv2d: 2-15                           1,327,104\n",
      "|    └─BatchNorm2d: 2-16                      768\n",
      "|    └─MultiStepLIFNode: 2-17                 --\n",
      "|    |    └─Sigmoid: 3-5                      --\n",
      "├─ModuleList: 1-2                             --\n",
      "|    └─Block: 2-18                            --\n",
      "|    |    └─LayerNorm: 3-6                    768\n",
      "|    |    └─SSA: 3-7                          594,432\n",
      "|    |    └─LayerNorm: 3-8                    768\n",
      "|    |    └─MLP: 3-9                          1,185,408\n",
      "|    └─Block: 2-19                            --\n",
      "|    |    └─LayerNorm: 3-10                   768\n",
      "|    |    └─SSA: 3-11                         594,432\n",
      "|    |    └─LayerNorm: 3-12                   768\n",
      "|    |    └─MLP: 3-13                         1,185,408\n",
      "|    └─Block: 2-20                            --\n",
      "|    |    └─LayerNorm: 3-14                   768\n",
      "|    |    └─SSA: 3-15                         594,432\n",
      "|    |    └─LayerNorm: 3-16                   768\n",
      "|    |    └─MLP: 3-17                         1,185,408\n",
      "|    └─Block: 2-21                            --\n",
      "|    |    └─LayerNorm: 3-18                   768\n",
      "|    |    └─SSA: 3-19                         594,432\n",
      "|    |    └─LayerNorm: 3-20                   768\n",
      "|    |    └─MLP: 3-21                         1,185,408\n",
      "├─Linear: 1-3                                 3,850\n",
      "======================================================================\n",
      "Total params: 9,330,874\n",
      "Trainable params: 9,330,874\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─SPS: 1-1                                    --\n",
       "|    └─Conv2d: 2-1                            1,296\n",
       "|    └─BatchNorm2d: 2-2                       96\n",
       "|    └─MultiStepLIFNode: 2-3                  --\n",
       "|    |    └─Sigmoid: 3-1                      --\n",
       "|    └─Conv2d: 2-4                            41,472\n",
       "|    └─BatchNorm2d: 2-5                       192\n",
       "|    └─MultiStepLIFNode: 2-6                  --\n",
       "|    |    └─Sigmoid: 3-2                      --\n",
       "|    └─Conv2d: 2-7                            165,888\n",
       "|    └─BatchNorm2d: 2-8                       384\n",
       "|    └─MultiStepLIFNode: 2-9                  --\n",
       "|    |    └─Sigmoid: 3-3                      --\n",
       "|    └─MaxPool2d: 2-10                        --\n",
       "|    └─Conv2d: 2-11                           663,552\n",
       "|    └─BatchNorm2d: 2-12                      768\n",
       "|    └─MultiStepLIFNode: 2-13                 --\n",
       "|    |    └─Sigmoid: 3-4                      --\n",
       "|    └─MaxPool2d: 2-14                        --\n",
       "|    └─Conv2d: 2-15                           1,327,104\n",
       "|    └─BatchNorm2d: 2-16                      768\n",
       "|    └─MultiStepLIFNode: 2-17                 --\n",
       "|    |    └─Sigmoid: 3-5                      --\n",
       "├─ModuleList: 1-2                             --\n",
       "|    └─Block: 2-18                            --\n",
       "|    |    └─LayerNorm: 3-6                    768\n",
       "|    |    └─SSA: 3-7                          594,432\n",
       "|    |    └─LayerNorm: 3-8                    768\n",
       "|    |    └─MLP: 3-9                          1,185,408\n",
       "|    └─Block: 2-19                            --\n",
       "|    |    └─LayerNorm: 3-10                   768\n",
       "|    |    └─SSA: 3-11                         594,432\n",
       "|    |    └─LayerNorm: 3-12                   768\n",
       "|    |    └─MLP: 3-13                         1,185,408\n",
       "|    └─Block: 2-20                            --\n",
       "|    |    └─LayerNorm: 3-14                   768\n",
       "|    |    └─SSA: 3-15                         594,432\n",
       "|    |    └─LayerNorm: 3-16                   768\n",
       "|    |    └─MLP: 3-17                         1,185,408\n",
       "|    └─Block: 2-21                            --\n",
       "|    |    └─LayerNorm: 3-18                   768\n",
       "|    |    └─SSA: 3-19                         594,432\n",
       "|    |    └─LayerNorm: 3-20                   768\n",
       "|    |    └─MLP: 3-21                         1,185,408\n",
       "├─Linear: 1-3                                 3,850\n",
       "======================================================================\n",
       "Total params: 9,330,874\n",
       "Trainable params: 9,330,874\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0389c52c-e45a-45b7-b38c-840450c9e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: check the correctness of folding \n",
    "class BN_Folder():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def fold(self, model):\n",
    "\n",
    "        new_model = copy.deepcopy(model)\n",
    "\n",
    "        module_names = list(new_model._modules)\n",
    "\n",
    "        for k, name in enumerate(module_names):\n",
    "\n",
    "            if len(list(new_model._modules[name]._modules)) > 0:\n",
    "                \n",
    "                new_model._modules[name] = self.fold(new_model._modules[name])\n",
    "\n",
    "            else:\n",
    "                if isinstance(new_model._modules[name], nn.BatchNorm2d):\n",
    "                    if isinstance(new_model._modules[module_names[k-1]], nn.Conv2d):\n",
    "\n",
    "                        # Folded BN\n",
    "                        folded_conv = self._fold_conv_bn_eval(new_model._modules[module_names[k-1]], new_model._modules[name])\n",
    "\n",
    "                        # Replace old weight values\n",
    "                        #new_model._modules.pop(name) # Remove the BN layer\n",
    "                        new_model._modules[module_names[k]] = nn.Identity()\n",
    "                        new_model._modules[module_names[k-1]] = folded_conv # Replace the Convolutional Layer by the folded version\n",
    "                        \n",
    "                if isinstance(new_model._modules[name], nn.BatchNorm1d):\n",
    "                    if isinstance(new_model._modules[module_names[k-1]], nn.Linear) :\n",
    "                        # Folded BN\n",
    "                        folded_conv = self._fold_conv_bn_eval(new_model._modules[module_names[k-1]], new_model._modules[name])\n",
    "\n",
    "                        # Replace old weight values\n",
    "                        #new_model._modules.pop(name) # Remove the BN layer\n",
    "                        new_model._modules[module_names[k]] = nn.Identity()\n",
    "                        new_model._modules[module_names[k-1]] = folded_conv # Replace the Convolutional Layer by the folded version\n",
    "\n",
    "        return new_model\n",
    "\n",
    "\n",
    "    def _bn_folding(self, conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):\n",
    "        if conv_b is None:\n",
    "            conv_b = bn_rm.new_zeros(bn_rm.shape)\n",
    "        bn_var_rsqrt = torch.rsqrt(bn_rv + bn_eps)\n",
    "\n",
    "        w_fold = conv_w * (bn_w * bn_var_rsqrt).view(-1, 1, 1, 1)\n",
    "        b_fold = (conv_b - bn_rm) * bn_var_rsqrt * bn_w + bn_b\n",
    "\n",
    "        return torch.nn.Parameter(w_fold), torch.nn.Parameter(b_fold)\n",
    "\n",
    "\n",
    "    def _bn_folding_linear(self, conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):\n",
    "        if conv_b is None:\n",
    "            conv_b = bn_rm.new_zeros(bn_rm.shape)\n",
    "        \n",
    "        bn_var_rsqrt = torch.rsqrt(bn_rv + bn_eps)\n",
    "\n",
    "        print(\"Before: \",conv_w.shape, conv_b.shape)\n",
    "        w_fold = conv_w * (bn_w * bn_var_rsqrt).view(-1, 1)\n",
    "        b_fold = (conv_b - bn_rm) * bn_var_rsqrt * bn_w + bn_b\n",
    "        print(\"After: \",w_fold.shape, b_fold.shape)\n",
    "        \n",
    "        return torch.nn.Parameter(w_fold), torch.nn.Parameter(b_fold)\n",
    "\n",
    "        \n",
    "    def _fold_conv_bn_eval(self, conv, bn):\n",
    "        assert(not (conv.training or bn.training)), \"Fusion only for eval!\"\n",
    "        fused_conv = copy.deepcopy(conv)\n",
    "        \n",
    "        if isinstance(bn, nn.BatchNorm1d):\n",
    "            fused_conv.weight, fused_conv.bias = self._bn_folding_linear(fused_conv.weight, fused_conv.bias,\n",
    "                                     bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)\n",
    "        else:\n",
    "            fused_conv.weight, fused_conv.bias = self._bn_folding(fused_conv.weight, fused_conv.bias,\n",
    "                                     bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)\n",
    "\n",
    "        return fused_conv\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51615fb2-ceae-4cb4-88f6-3e6383f38f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([1536, 384]) torch.Size([1536])\n",
      "After:  torch.Size([1536, 384]) torch.Size([1536])\n",
      "Before:  torch.Size([384, 1536]) torch.Size([384])\n",
      "After:  torch.Size([384, 1536]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([1536, 384]) torch.Size([1536])\n",
      "After:  torch.Size([1536, 384]) torch.Size([1536])\n",
      "Before:  torch.Size([384, 1536]) torch.Size([384])\n",
      "After:  torch.Size([384, 1536]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([1536, 384]) torch.Size([1536])\n",
      "After:  torch.Size([1536, 384]) torch.Size([1536])\n",
      "Before:  torch.Size([384, 1536]) torch.Size([384])\n",
      "After:  torch.Size([384, 1536]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([384, 384]) torch.Size([384])\n",
      "After:  torch.Size([384, 384]) torch.Size([384])\n",
      "Before:  torch.Size([1536, 384]) torch.Size([1536])\n",
      "After:  torch.Size([1536, 384]) torch.Size([1536])\n",
      "Before:  torch.Size([384, 1536]) torch.Size([384])\n",
      "After:  torch.Size([384, 1536]) torch.Size([384])\n",
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─SPS: 1-1                                    --\n",
      "|    └─Conv2d: 2-1                            1,344\n",
      "|    └─Identity: 2-2                          --\n",
      "|    └─MultiStepLIFNode: 2-3                  --\n",
      "|    |    └─Sigmoid: 3-1                      --\n",
      "|    └─Conv2d: 2-4                            41,568\n",
      "|    └─Identity: 2-5                          --\n",
      "|    └─MultiStepLIFNode: 2-6                  --\n",
      "|    |    └─Sigmoid: 3-2                      --\n",
      "|    └─Conv2d: 2-7                            166,080\n",
      "|    └─Identity: 2-8                          --\n",
      "|    └─MultiStepLIFNode: 2-9                  --\n",
      "|    |    └─Sigmoid: 3-3                      --\n",
      "|    └─MaxPool2d: 2-10                        --\n",
      "|    └─Conv2d: 2-11                           663,936\n",
      "|    └─Identity: 2-12                         --\n",
      "|    └─MultiStepLIFNode: 2-13                 --\n",
      "|    |    └─Sigmoid: 3-4                      --\n",
      "|    └─MaxPool2d: 2-14                        --\n",
      "|    └─Conv2d: 2-15                           1,327,488\n",
      "|    └─Identity: 2-16                         --\n",
      "|    └─MultiStepLIFNode: 2-17                 --\n",
      "|    |    └─Sigmoid: 3-5                      --\n",
      "├─ModuleList: 1-2                             --\n",
      "|    └─Block: 2-18                            --\n",
      "|    |    └─LayerNorm: 3-6                    768\n",
      "|    |    └─SSA: 3-7                          591,360\n",
      "|    |    └─LayerNorm: 3-8                    768\n",
      "|    |    └─MLP: 3-9                          1,181,568\n",
      "|    └─Block: 2-19                            --\n",
      "|    |    └─LayerNorm: 3-10                   768\n",
      "|    |    └─SSA: 3-11                         591,360\n",
      "|    |    └─LayerNorm: 3-12                   768\n",
      "|    |    └─MLP: 3-13                         1,181,568\n",
      "|    └─Block: 2-20                            --\n",
      "|    |    └─LayerNorm: 3-14                   768\n",
      "|    |    └─SSA: 3-15                         591,360\n",
      "|    |    └─LayerNorm: 3-16                   768\n",
      "|    |    └─MLP: 3-17                         1,181,568\n",
      "|    └─Block: 2-21                            --\n",
      "|    |    └─LayerNorm: 3-18                   768\n",
      "|    |    └─SSA: 3-19                         591,360\n",
      "|    |    └─LayerNorm: 3-20                   768\n",
      "|    |    └─MLP: 3-21                         1,181,568\n",
      "├─Linear: 1-3                                 3,850\n",
      "======================================================================\n",
      "Total params: 9,302,122\n",
      "Trainable params: 9,302,122\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─SPS: 1-1                                    --\n",
       "|    └─Conv2d: 2-1                            1,344\n",
       "|    └─Identity: 2-2                          --\n",
       "|    └─MultiStepLIFNode: 2-3                  --\n",
       "|    |    └─Sigmoid: 3-1                      --\n",
       "|    └─Conv2d: 2-4                            41,568\n",
       "|    └─Identity: 2-5                          --\n",
       "|    └─MultiStepLIFNode: 2-6                  --\n",
       "|    |    └─Sigmoid: 3-2                      --\n",
       "|    └─Conv2d: 2-7                            166,080\n",
       "|    └─Identity: 2-8                          --\n",
       "|    └─MultiStepLIFNode: 2-9                  --\n",
       "|    |    └─Sigmoid: 3-3                      --\n",
       "|    └─MaxPool2d: 2-10                        --\n",
       "|    └─Conv2d: 2-11                           663,936\n",
       "|    └─Identity: 2-12                         --\n",
       "|    └─MultiStepLIFNode: 2-13                 --\n",
       "|    |    └─Sigmoid: 3-4                      --\n",
       "|    └─MaxPool2d: 2-14                        --\n",
       "|    └─Conv2d: 2-15                           1,327,488\n",
       "|    └─Identity: 2-16                         --\n",
       "|    └─MultiStepLIFNode: 2-17                 --\n",
       "|    |    └─Sigmoid: 3-5                      --\n",
       "├─ModuleList: 1-2                             --\n",
       "|    └─Block: 2-18                            --\n",
       "|    |    └─LayerNorm: 3-6                    768\n",
       "|    |    └─SSA: 3-7                          591,360\n",
       "|    |    └─LayerNorm: 3-8                    768\n",
       "|    |    └─MLP: 3-9                          1,181,568\n",
       "|    └─Block: 2-19                            --\n",
       "|    |    └─LayerNorm: 3-10                   768\n",
       "|    |    └─SSA: 3-11                         591,360\n",
       "|    |    └─LayerNorm: 3-12                   768\n",
       "|    |    └─MLP: 3-13                         1,181,568\n",
       "|    └─Block: 2-20                            --\n",
       "|    |    └─LayerNorm: 3-14                   768\n",
       "|    |    └─SSA: 3-15                         591,360\n",
       "|    |    └─LayerNorm: 3-16                   768\n",
       "|    |    └─MLP: 3-17                         1,181,568\n",
       "|    └─Block: 2-21                            --\n",
       "|    |    └─LayerNorm: 3-18                   768\n",
       "|    |    └─SSA: 3-19                         591,360\n",
       "|    |    └─LayerNorm: 3-20                   768\n",
       "|    |    └─MLP: 3-21                         1,181,568\n",
       "├─Linear: 1-3                                 3,850\n",
       "======================================================================\n",
       "Total params: 9,302,122\n",
       "Trainable params: 9,302,122\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "bn = BN_Folder()\n",
    "model = bn.fold(net)\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9f6ad-5fdd-4858-ad8e-5dc9afaa357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test the accuracy\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()\n",
    "# batch_accuracy(test_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd3c57-9114-4661-8473-b00dedf41aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Mapping to CRI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
